{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para mapear posiciones a índices\n",
    "position_to_index = {\n",
    "    '5050_guard': 0,\n",
    "    'back1': 1,\n",
    "    'back2': 2,\n",
    "    'closed_guard1': 3,\n",
    "    'closed_guard2': 4,\n",
    "    'half_guard1': 5,\n",
    "    'half_guard2': 6,\n",
    "    'mount1': 7,\n",
    "    'mount2': 8,\n",
    "    'open_guard1': 9,\n",
    "    'open_guard2': 10,\n",
    "    'side_control1': 11,\n",
    "    'side_control2': 12,\n",
    "    'standing': 13,\n",
    "    'takedown1': 14,\n",
    "    'takedown2': 15,\n",
    "    'turtle1': 16,\n",
    "    'turtle2': 17\n",
    "}\n",
    "\n",
    "# Transformaciones para las imágenes\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def remove_background(image):\n",
    "    \"\"\"\n",
    "    Quita el fondo de una imagen utilizando segmentación de contornos con OpenCV.\n",
    "    \"\"\"\n",
    "    image_np = np.array(image)\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros_like(gray)\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, 255, thickness=cv2.FILLED)\n",
    "    result = cv2.bitwise_and(image_np, image_np, mask=mask)\n",
    "    return Image.fromarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(file_path):\n",
    "    \"\"\"\n",
    "    Carga las anotaciones preprocesadas desde un archivo JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BJJDataset(Dataset):\n",
    "    def __init__(self, annotations, image_dir, transform=None):\n",
    "        self.annotations = annotations\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.annotations[idx]['Image'] + '.jpg'\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # Aplicar eliminación de fondo antes de transformaciones\n",
    "        image = remove_background(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_str = self.annotations[idx]['Position']\n",
    "        label = position_to_index[label_str]\n",
    "        label = torch.tensor(label).long()\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes, device):\n",
    "    \"\"\"\n",
    "    Inicializa el modelo ResNet-18 y lo prepara para entrenamiento.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Realiza una época de entrenamiento en el modelo.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    return running_loss / len(train_loader.dataset), correct.double() / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Realiza una época de validación en el modelo y calcula métricas adicionales.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_acc = val_correct.double() / len(val_loader.dataset)\n",
    "\n",
    "    # Calcular métricas\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    std_ae = np.std(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "    print(f\"Validation Metrics -> Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, \"\n",
    "          f\"MAE: {mae:.4f}, MSE: {mse:.4f}, R²: {r2:.4f}, Std_AE: {std_ae:.4f}\")\n",
    "\n",
    "    return val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_kfold(dataset, num_classes, k_folds=5, num_epochs=25, batch_size=32, early_stopping_patience=5):\n",
    "    \"\"\"\n",
    "    Entrena el modelo ResNet-18 utilizando validación cruzada con K-Fold.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "\n",
    "        # Dividir dataset en subconjuntos de entrenamiento y validación\n",
    "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size)\n",
    "\n",
    "        # Inicializar modelo, criterio, optimizador y scheduler\n",
    "        model = initialize_model(num_classes, device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Entrenamiento\n",
    "            epoch_loss, epoch_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "            # Validación\n",
    "            val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "            print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "            # Early Stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stopping_counter = 0\n",
    "                torch.save(model.state_dict(), f'model_fold_{fold+1}.pth')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                if early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/25, Loss: 1.4633, Accuracy: 0.4981\n",
      "Validation Metrics -> Loss: 1.1553, Accuracy: 0.6013, MAE: 1.8845, MSE: 15.4628, R²: 0.4183, Std_AE: 3.4513\n",
      "Validation Loss: 1.1553, Validation Accuracy: 0.6013\n",
      "Epoch 2/25, Loss: 0.9426, Accuracy: 0.6696\n",
      "Validation Metrics -> Loss: 0.8491, Accuracy: 0.7112, MAE: 1.5359, MSE: 13.5476, R²: 0.4904, Std_AE: 3.3449\n",
      "Validation Loss: 0.8491, Validation Accuracy: 0.7112\n",
      "Epoch 3/25, Loss: 0.6865, Accuracy: 0.7605\n",
      "Validation Metrics -> Loss: 0.6913, Accuracy: 0.7646, MAE: 1.1843, MSE: 10.0137, R²: 0.6233, Std_AE: 2.9345\n",
      "Validation Loss: 0.6913, Validation Accuracy: 0.7646\n",
      "Epoch 4/25, Loss: 0.5233, Accuracy: 0.8148\n",
      "Validation Metrics -> Loss: 0.5613, Accuracy: 0.8123, MAE: 0.9074, MSE: 7.4245, R²: 0.7207, Std_AE: 2.5693\n",
      "Validation Loss: 0.5613, Validation Accuracy: 0.8123\n",
      "Epoch 5/25, Loss: 0.4055, Accuracy: 0.8556\n",
      "Validation Metrics -> Loss: 0.5345, Accuracy: 0.8247, MAE: 0.8067, MSE: 6.4180, R²: 0.7586, Std_AE: 2.4015\n",
      "Validation Loss: 0.5345, Validation Accuracy: 0.8247\n",
      "Epoch 6/25, Loss: 0.3273, Accuracy: 0.8814\n",
      "Validation Metrics -> Loss: 0.5656, Accuracy: 0.8255, MAE: 0.7581, MSE: 5.8241, R²: 0.7809, Std_AE: 2.2911\n",
      "Validation Loss: 0.5656, Validation Accuracy: 0.8255\n",
      "Epoch 7/25, Loss: 0.2692, Accuracy: 0.9027\n",
      "Validation Metrics -> Loss: 0.4801, Accuracy: 0.8528, MAE: 0.6946, MSE: 5.7938, R²: 0.7821, Std_AE: 2.3046\n",
      "Validation Loss: 0.4801, Validation Accuracy: 0.8528\n",
      "Epoch 8/25, Loss: 0.2312, Accuracy: 0.9157\n",
      "Validation Metrics -> Loss: 0.5051, Accuracy: 0.8560, MAE: 0.6985, MSE: 5.8219, R²: 0.7810, Std_AE: 2.3095\n",
      "Validation Loss: 0.5051, Validation Accuracy: 0.8560\n",
      "Epoch 9/25, Loss: 0.2014, Accuracy: 0.9276\n",
      "Validation Metrics -> Loss: 0.5380, Accuracy: 0.8514, MAE: 0.7212, MSE: 5.9642, R²: 0.7756, Std_AE: 2.3333\n",
      "Validation Loss: 0.5380, Validation Accuracy: 0.8514\n",
      "Epoch 10/25, Loss: 0.1781, Accuracy: 0.9343\n",
      "Validation Metrics -> Loss: 0.5432, Accuracy: 0.8552, MAE: 0.6650, MSE: 5.2316, R²: 0.8032, Std_AE: 2.1885\n",
      "Validation Loss: 0.5432, Validation Accuracy: 0.8552\n",
      "Epoch 11/25, Loss: 0.1609, Accuracy: 0.9426\n",
      "Validation Metrics -> Loss: 0.5036, Accuracy: 0.8718, MAE: 0.5920, MSE: 4.6875, R²: 0.8237, Std_AE: 2.0826\n",
      "Validation Loss: 0.5036, Validation Accuracy: 0.8718\n",
      "Epoch 12/25, Loss: 0.1420, Accuracy: 0.9497\n",
      "Validation Metrics -> Loss: 0.5403, Accuracy: 0.8659, MAE: 0.6320, MSE: 5.1307, R²: 0.8070, Std_AE: 2.1751\n",
      "Validation Loss: 0.5403, Validation Accuracy: 0.8659\n",
      "Early stopping\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.4334, Accuracy: 0.5116\n",
      "Validation Metrics -> Loss: 1.1944, Accuracy: 0.5860, MAE: 1.8839, MSE: 14.9321, R²: 0.4415, Std_AE: 3.3738\n",
      "Validation Loss: 1.1944, Validation Accuracy: 0.5860\n",
      "Epoch 2/25, Loss: 0.9404, Accuracy: 0.6729\n",
      "Validation Metrics -> Loss: 0.8029, Accuracy: 0.7216, MAE: 1.3411, MSE: 11.0276, R²: 0.5875, Std_AE: 3.0379\n",
      "Validation Loss: 0.8029, Validation Accuracy: 0.7216\n",
      "Epoch 3/25, Loss: 0.6841, Accuracy: 0.7606\n",
      "Validation Metrics -> Loss: 0.6416, Accuracy: 0.7767, MAE: 1.0368, MSE: 8.2948, R²: 0.6898, Std_AE: 2.6870\n",
      "Validation Loss: 0.6416, Validation Accuracy: 0.7767\n",
      "Epoch 4/25, Loss: 0.5175, Accuracy: 0.8165\n",
      "Validation Metrics -> Loss: 0.5977, Accuracy: 0.7979, MAE: 0.9775, MSE: 8.1225, R²: 0.6962, Std_AE: 2.6771\n",
      "Validation Loss: 0.5977, Validation Accuracy: 0.7979\n",
      "Epoch 5/25, Loss: 0.4100, Accuracy: 0.8543\n",
      "Validation Metrics -> Loss: 0.5118, Accuracy: 0.8299, MAE: 0.8393, MSE: 7.0409, R²: 0.7367, Std_AE: 2.5172\n",
      "Validation Loss: 0.5118, Validation Accuracy: 0.8299\n",
      "Epoch 6/25, Loss: 0.3294, Accuracy: 0.8826\n",
      "Validation Metrics -> Loss: 0.5514, Accuracy: 0.8313, MAE: 0.8431, MSE: 7.1041, R²: 0.7343, Std_AE: 2.5285\n",
      "Validation Loss: 0.5514, Validation Accuracy: 0.8313\n",
      "Epoch 7/25, Loss: 0.2740, Accuracy: 0.9019\n",
      "Validation Metrics -> Loss: 0.4880, Accuracy: 0.8502, MAE: 0.7236, MSE: 6.0555, R²: 0.7735, Std_AE: 2.3520\n",
      "Validation Loss: 0.4880, Validation Accuracy: 0.8502\n",
      "Epoch 8/25, Loss: 0.2338, Accuracy: 0.9163\n",
      "Validation Metrics -> Loss: 0.4799, Accuracy: 0.8591, MAE: 0.6812, MSE: 5.6959, R²: 0.7870, Std_AE: 2.2873\n",
      "Validation Loss: 0.4799, Validation Accuracy: 0.8591\n",
      "Epoch 9/25, Loss: 0.2002, Accuracy: 0.9281\n",
      "Validation Metrics -> Loss: 0.4689, Accuracy: 0.8669, MAE: 0.6536, MSE: 5.4390, R²: 0.7966, Std_AE: 2.2387\n",
      "Validation Loss: 0.4689, Validation Accuracy: 0.8669\n",
      "Epoch 10/25, Loss: 0.1794, Accuracy: 0.9349\n",
      "Validation Metrics -> Loss: 0.5010, Accuracy: 0.8597, MAE: 0.6695, MSE: 5.5436, R²: 0.7927, Std_AE: 2.2573\n",
      "Validation Loss: 0.5010, Validation Accuracy: 0.8597\n",
      "Epoch 11/25, Loss: 0.1556, Accuracy: 0.9433\n",
      "Validation Metrics -> Loss: 0.5116, Accuracy: 0.8659, MAE: 0.6516, MSE: 5.4210, R²: 0.7972, Std_AE: 2.2353\n",
      "Validation Loss: 0.5116, Validation Accuracy: 0.8659\n",
      "Epoch 12/25, Loss: 0.1394, Accuracy: 0.9499\n",
      "Validation Metrics -> Loss: 0.5612, Accuracy: 0.8662, MAE: 0.6458, MSE: 5.3945, R²: 0.7982, Std_AE: 2.2310\n",
      "Validation Loss: 0.5612, Validation Accuracy: 0.8662\n",
      "Epoch 13/25, Loss: 0.1234, Accuracy: 0.9557\n",
      "Validation Metrics -> Loss: 0.5077, Accuracy: 0.8711, MAE: 0.6464, MSE: 5.6311, R²: 0.7894, Std_AE: 2.2833\n",
      "Validation Loss: 0.5077, Validation Accuracy: 0.8711\n",
      "Epoch 14/25, Loss: 0.1131, Accuracy: 0.9596\n",
      "Validation Metrics -> Loss: 0.5429, Accuracy: 0.8669, MAE: 0.6384, MSE: 5.3105, R²: 0.8014, Std_AE: 2.2143\n",
      "Validation Loss: 0.5429, Validation Accuracy: 0.8669\n",
      "Early stopping\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.4410, Accuracy: 0.5086\n",
      "Validation Metrics -> Loss: 1.0736, Accuracy: 0.6284, MAE: 1.7248, MSE: 14.1760, R²: 0.4661, Std_AE: 3.3468\n",
      "Validation Loss: 1.0736, Validation Accuracy: 0.6284\n",
      "Epoch 2/25, Loss: 0.9265, Accuracy: 0.6791\n",
      "Validation Metrics -> Loss: 0.8368, Accuracy: 0.7142, MAE: 1.3934, MSE: 11.8316, R²: 0.5544, Std_AE: 3.1448\n",
      "Validation Loss: 0.8368, Validation Accuracy: 0.7142\n",
      "Epoch 3/25, Loss: 0.6677, Accuracy: 0.7691\n",
      "Validation Metrics -> Loss: 0.6642, Accuracy: 0.7701, MAE: 1.0825, MSE: 9.0004, R²: 0.6611, Std_AE: 2.7980\n",
      "Validation Loss: 0.6642, Validation Accuracy: 0.7701\n",
      "Epoch 4/25, Loss: 0.5093, Accuracy: 0.8240\n",
      "Validation Metrics -> Loss: 0.6136, Accuracy: 0.7939, MAE: 0.9895, MSE: 8.2726, R²: 0.6885, Std_AE: 2.7006\n",
      "Validation Loss: 0.6136, Validation Accuracy: 0.7939\n",
      "Epoch 5/25, Loss: 0.3987, Accuracy: 0.8595\n",
      "Validation Metrics -> Loss: 0.5721, Accuracy: 0.8144, MAE: 0.8502, MSE: 6.7695, R²: 0.7451, Std_AE: 2.4590\n",
      "Validation Loss: 0.5721, Validation Accuracy: 0.8144\n",
      "Epoch 6/25, Loss: 0.3261, Accuracy: 0.8834\n",
      "Validation Metrics -> Loss: 0.5464, Accuracy: 0.8278, MAE: 0.7894, MSE: 6.4544, R²: 0.7569, Std_AE: 2.4148\n",
      "Validation Loss: 0.5464, Validation Accuracy: 0.8278\n",
      "Epoch 7/25, Loss: 0.2646, Accuracy: 0.9058\n",
      "Validation Metrics -> Loss: 0.5412, Accuracy: 0.8353, MAE: 0.7584, MSE: 5.9725, R²: 0.7751, Std_AE: 2.3232\n",
      "Validation Loss: 0.5412, Validation Accuracy: 0.8353\n",
      "Epoch 8/25, Loss: 0.2346, Accuracy: 0.9155\n",
      "Validation Metrics -> Loss: 0.5061, Accuracy: 0.8530, MAE: 0.6805, MSE: 5.3506, R²: 0.7985, Std_AE: 2.2108\n",
      "Validation Loss: 0.5061, Validation Accuracy: 0.8530\n",
      "Epoch 9/25, Loss: 0.1989, Accuracy: 0.9278\n",
      "Validation Metrics -> Loss: 0.5146, Accuracy: 0.8544, MAE: 0.6778, MSE: 5.5034, R²: 0.7927, Std_AE: 2.2459\n",
      "Validation Loss: 0.5146, Validation Accuracy: 0.8544\n",
      "Epoch 10/25, Loss: 0.1738, Accuracy: 0.9371\n",
      "Validation Metrics -> Loss: 0.5201, Accuracy: 0.8578, MAE: 0.6786, MSE: 5.6858, R²: 0.7859, Std_AE: 2.2859\n",
      "Validation Loss: 0.5201, Validation Accuracy: 0.8578\n",
      "Epoch 11/25, Loss: 0.1577, Accuracy: 0.9426\n",
      "Validation Metrics -> Loss: 0.5266, Accuracy: 0.8587, MAE: 0.6471, MSE: 5.1666, R²: 0.8054, Std_AE: 2.1790\n",
      "Validation Loss: 0.5266, Validation Accuracy: 0.8587\n",
      "Epoch 12/25, Loss: 0.1388, Accuracy: 0.9496\n",
      "Validation Metrics -> Loss: 0.5367, Accuracy: 0.8701, MAE: 0.6078, MSE: 4.9797, R²: 0.8125, Std_AE: 2.1471\n",
      "Validation Loss: 0.5367, Validation Accuracy: 0.8701\n",
      "Epoch 13/25, Loss: 0.1256, Accuracy: 0.9552\n",
      "Validation Metrics -> Loss: 0.6088, Accuracy: 0.8549, MAE: 0.6420, MSE: 4.9585, R²: 0.8133, Std_AE: 2.1322\n",
      "Validation Loss: 0.6088, Validation Accuracy: 0.8549\n",
      "Early stopping\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.4437, Accuracy: 0.5052\n",
      "Validation Metrics -> Loss: 1.1250, Accuracy: 0.6121, MAE: 1.8626, MSE: 15.3110, R²: 0.4406, Std_AE: 3.4412\n",
      "Validation Loss: 1.1250, Validation Accuracy: 0.6121\n",
      "Epoch 2/25, Loss: 0.9437, Accuracy: 0.6712\n",
      "Validation Metrics -> Loss: 0.8040, Accuracy: 0.7186, MAE: 1.2706, MSE: 10.5152, R²: 0.6158, Std_AE: 2.9834\n",
      "Validation Loss: 0.8040, Validation Accuracy: 0.7186\n",
      "Epoch 3/25, Loss: 0.6853, Accuracy: 0.7642\n",
      "Validation Metrics -> Loss: 0.6559, Accuracy: 0.7704, MAE: 1.1514, MSE: 9.9055, R²: 0.6381, Std_AE: 2.9291\n",
      "Validation Loss: 0.6559, Validation Accuracy: 0.7704\n",
      "Epoch 4/25, Loss: 0.5117, Accuracy: 0.8218\n",
      "Validation Metrics -> Loss: 0.5344, Accuracy: 0.8173, MAE: 0.8721, MSE: 7.3270, R²: 0.7323, Std_AE: 2.5625\n",
      "Validation Loss: 0.5344, Validation Accuracy: 0.8173\n",
      "Epoch 5/25, Loss: 0.3969, Accuracy: 0.8581\n",
      "Validation Metrics -> Loss: 0.5112, Accuracy: 0.8357, MAE: 0.7575, MSE: 6.0498, R²: 0.7790, Std_AE: 2.3401\n",
      "Validation Loss: 0.5112, Validation Accuracy: 0.8357\n",
      "Epoch 6/25, Loss: 0.3152, Accuracy: 0.8856\n",
      "Validation Metrics -> Loss: 0.4734, Accuracy: 0.8498, MAE: 0.6820, MSE: 5.4352, R²: 0.8014, Std_AE: 2.2294\n",
      "Validation Loss: 0.4734, Validation Accuracy: 0.8498\n",
      "Epoch 7/25, Loss: 0.2627, Accuracy: 0.9047\n",
      "Validation Metrics -> Loss: 0.4593, Accuracy: 0.8566, MAE: 0.6580, MSE: 5.4120, R²: 0.8023, Std_AE: 2.2314\n",
      "Validation Loss: 0.4593, Validation Accuracy: 0.8566\n",
      "Epoch 8/25, Loss: 0.2263, Accuracy: 0.9170\n",
      "Validation Metrics -> Loss: 0.4920, Accuracy: 0.8549, MAE: 0.7041, MSE: 6.0663, R²: 0.7784, Std_AE: 2.3602\n",
      "Validation Loss: 0.4920, Validation Accuracy: 0.8549\n",
      "Epoch 9/25, Loss: 0.1939, Accuracy: 0.9294\n",
      "Validation Metrics -> Loss: 0.4936, Accuracy: 0.8613, MAE: 0.6398, MSE: 5.1306, R²: 0.8125, Std_AE: 2.1729\n",
      "Validation Loss: 0.4936, Validation Accuracy: 0.8613\n",
      "Epoch 10/25, Loss: 0.1726, Accuracy: 0.9370\n",
      "Validation Metrics -> Loss: 0.5558, Accuracy: 0.8518, MAE: 0.6975, MSE: 5.7391, R²: 0.7903, Std_AE: 2.2918\n",
      "Validation Loss: 0.5558, Validation Accuracy: 0.8518\n",
      "Epoch 11/25, Loss: 0.1508, Accuracy: 0.9453\n",
      "Validation Metrics -> Loss: 0.5142, Accuracy: 0.8635, MAE: 0.6166, MSE: 5.0327, R²: 0.8161, Std_AE: 2.1569\n",
      "Validation Loss: 0.5142, Validation Accuracy: 0.8635\n",
      "Epoch 12/25, Loss: 0.1386, Accuracy: 0.9499\n",
      "Validation Metrics -> Loss: 0.5194, Accuracy: 0.8713, MAE: 0.5764, MSE: 4.4974, R²: 0.8357, Std_AE: 2.0409\n",
      "Validation Loss: 0.5194, Validation Accuracy: 0.8713\n",
      "Early stopping\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tuf Gaming\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.4560, Accuracy: 0.5006\n",
      "Validation Metrics -> Loss: 1.1830, Accuracy: 0.5924, MAE: 1.9608, MSE: 16.2488, R²: 0.4004, Std_AE: 3.5220\n",
      "Validation Loss: 1.1830, Validation Accuracy: 0.5924\n",
      "Epoch 2/25, Loss: 0.9643, Accuracy: 0.6673\n",
      "Validation Metrics -> Loss: 0.7663, Accuracy: 0.7369, MAE: 1.2541, MSE: 10.2808, R²: 0.6206, Std_AE: 2.9510\n",
      "Validation Loss: 0.7663, Validation Accuracy: 0.7369\n",
      "Epoch 3/25, Loss: 0.7098, Accuracy: 0.7532\n",
      "Validation Metrics -> Loss: 0.6654, Accuracy: 0.7727, MAE: 1.0358, MSE: 8.3741, R²: 0.6910, Std_AE: 2.7021\n",
      "Validation Loss: 0.6654, Validation Accuracy: 0.7727\n",
      "Epoch 4/25, Loss: 0.5419, Accuracy: 0.8095\n",
      "Validation Metrics -> Loss: 0.6088, Accuracy: 0.7942, MAE: 0.9742, MSE: 8.2498, R²: 0.6956, Std_AE: 2.7020\n",
      "Validation Loss: 0.6088, Validation Accuracy: 0.7942\n",
      "Epoch 5/25, Loss: 0.4288, Accuracy: 0.8491\n",
      "Validation Metrics -> Loss: 0.5777, Accuracy: 0.8078, MAE: 0.9150, MSE: 7.4995, R²: 0.7233, Std_AE: 2.5811\n",
      "Validation Loss: 0.5777, Validation Accuracy: 0.8078\n",
      "Epoch 6/25, Loss: 0.3411, Accuracy: 0.8788\n",
      "Validation Metrics -> Loss: 0.5256, Accuracy: 0.8348, MAE: 0.8045, MSE: 6.7636, R²: 0.7504, Std_AE: 2.4731\n",
      "Validation Loss: 0.5256, Validation Accuracy: 0.8348\n",
      "Epoch 7/25, Loss: 0.2865, Accuracy: 0.8975\n",
      "Validation Metrics -> Loss: 0.4684, Accuracy: 0.8541, MAE: 0.7110, MSE: 5.8868, R²: 0.7828, Std_AE: 2.3198\n",
      "Validation Loss: 0.4684, Validation Accuracy: 0.8541\n",
      "Epoch 8/25, Loss: 0.2429, Accuracy: 0.9112\n",
      "Validation Metrics -> Loss: 0.5332, Accuracy: 0.8357, MAE: 0.7349, MSE: 5.8452, R²: 0.7843, Std_AE: 2.3033\n",
      "Validation Loss: 0.5332, Validation Accuracy: 0.8357\n",
      "Epoch 9/25, Loss: 0.2138, Accuracy: 0.9223\n",
      "Validation Metrics -> Loss: 0.5117, Accuracy: 0.8512, MAE: 0.7323, MSE: 6.2303, R²: 0.7701, Std_AE: 2.3862\n",
      "Validation Loss: 0.5117, Validation Accuracy: 0.8512\n",
      "Epoch 10/25, Loss: 0.1852, Accuracy: 0.9323\n",
      "Validation Metrics -> Loss: 0.5262, Accuracy: 0.8604, MAE: 0.6778, MSE: 5.6132, R²: 0.7929, Std_AE: 2.2702\n",
      "Validation Loss: 0.5262, Validation Accuracy: 0.8604\n",
      "Epoch 11/25, Loss: 0.1630, Accuracy: 0.9417\n",
      "Validation Metrics -> Loss: 0.5068, Accuracy: 0.8739, MAE: 0.6092, MSE: 5.1743, R²: 0.8091, Std_AE: 2.1916\n",
      "Validation Loss: 0.5068, Validation Accuracy: 0.8739\n",
      "Epoch 12/25, Loss: 0.1487, Accuracy: 0.9461\n",
      "Validation Metrics -> Loss: 0.5486, Accuracy: 0.8614, MAE: 0.6474, MSE: 5.2321, R²: 0.8069, Std_AE: 2.1939\n",
      "Validation Loss: 0.5486, Validation Accuracy: 0.8614\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal para cargar el dataset y entrenar el modelo con validación cruzada.\n",
    "    \"\"\"\n",
    "    annotations_path = '../mnt/V3/annotations/annotations_preprocessed.json'\n",
    "    image_dir = '../mnt/V3/images'\n",
    "\n",
    "    # Cargar anotaciones\n",
    "    annotations = load_annotations(annotations_path)\n",
    "\n",
    "    # Cargar el dataset completo\n",
    "    dataset = BJJDataset(annotations, image_dir, transform=data_transforms)\n",
    "\n",
    "    # Entrenar modelo con validación cruzada\n",
    "    train_model_kfold(dataset, num_classes=len(position_to_index))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
