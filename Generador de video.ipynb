{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './mnt/V2/annotations/annotations_preprocessed.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 123\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVideo guardado en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_video_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Ejecutar\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43mcreate_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 87\u001b[0m, in \u001b[0;36mcreate_video\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_video\u001b[39m():\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mannotations_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     88\u001b[0m         annotations \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     90\u001b[0m     fourcc \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoWriter_fourcc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp4v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './mnt/V2/annotations/annotations_preprocessed.json'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "# Configuración de dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformaciones de imagen\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Función para eliminar el fondo (como en el entrenamiento)\n",
    "def remove_background(image):\n",
    "    image_np = np.array(image)\n",
    "    gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    mask = np.zeros_like(gray)\n",
    "\n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        cv2.drawContours(mask, [largest_contour], -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    result = cv2.bitwise_and(image_np, image_np, mask=mask)\n",
    "    return Image.fromarray(result)\n",
    "\n",
    "# Modelo HRNet-W32 para predicción de poses\n",
    "class HRNetForPose(nn.Module):\n",
    "    def __init__(self, num_keypoints=102):\n",
    "        super(HRNetForPose, self).__init__()\n",
    "        self.backbone = deeplabv3_resnet50(pretrained=True).backbone\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, num_keypoints, kernel_size=1, stride=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)['out']\n",
    "        x = self.fc(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# Cargar pesos del modelo\n",
    "model = HRNetForPose().to(device)\n",
    "model.load_state_dict(torch.load(\"hrnet_best_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Inicializar filtro de Kalman con mayor suavizado\n",
    "def initialize_kalman():\n",
    "    kalman = cv2.KalmanFilter(6, 2)\n",
    "    kalman.measurementMatrix = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]], np.float32)\n",
    "    kalman.transitionMatrix = np.array([\n",
    "        [1, 0, 1, 0, 0.5, 0], [0, 1, 0, 1, 0, 0.5],\n",
    "        [0, 0, 1, 0, 1, 0], [0, 0, 0, 1, 0, 1],\n",
    "        [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]\n",
    "    ], np.float32)\n",
    "    kalman.processNoiseCov = np.eye(6, dtype=np.float32) * 0.0005  # Más suavidad\n",
    "    return kalman\n",
    "\n",
    "# Aplicar filtro de Kalman a coordenadas\n",
    "def apply_kalman_filter(kalman_filters, points):\n",
    "    filtered_points = []\n",
    "    num_joints = len(points) // 3\n",
    "\n",
    "    if len(kalman_filters) < num_joints:\n",
    "        kalman_filters.extend([initialize_kalman() for _ in range(num_joints - len(kalman_filters))])\n",
    "\n",
    "    for i in range(0, len(points), 3):\n",
    "        x, y, c = float(points[i]), float(points[i + 1]), float(points[i + 2])  # Asegurar que sean floats\n",
    "        if c > 0.5 and (i // 3) < len(kalman_filters):\n",
    "            measurement = np.array([[np.float32(x)], [np.float32(y)]])\n",
    "            kalman_filters[i // 3].correct(measurement)\n",
    "            predicted = kalman_filters[i // 3].predict()\n",
    "            filtered_points.extend([predicted[0][0], predicted[1][0], c])\n",
    "        else:\n",
    "            filtered_points.extend([x, y, c])\n",
    "    return filtered_points\n",
    "\n",
    "# Colores para los dos peleadores (Rojo y Azul)\n",
    "fighter_colors = [(0, 0, 255), (255, 0, 0)]  # Azul para peleador 1, Rojo para peleador 2\n",
    "\n",
    "# Dibujar solo puntos, sin uniones\n",
    "def draw_pose(image, pose, kalman_filters, colors):\n",
    "    pose = apply_kalman_filter(kalman_filters, pose)\n",
    "    num_joints = len(pose) // 3\n",
    "\n",
    "    for i in range(num_joints):\n",
    "        x, y, c = int(pose[i * 3] * 256), int(pose[i * 3 + 1] * 256), pose[i * 3 + 2]\n",
    "        if c > 0.5:\n",
    "            cv2.circle(image, (x, y), 2, colors[i // (num_joints // 2)], -1)  # Rojo para uno, Azul para el otro\n",
    "    return image\n",
    "\n",
    "# Directorios de imágenes\n",
    "annotations_path_v2 = \"./mnt/V2/annotations/annotations_preprocessed.json\"\n",
    "image_dir_v2 = \"./mnt/V2/images/\"\n",
    "image_dir_v3 = \"./mnt/V3/images/\"\n",
    "output_video_path = \"pose_comparison.mp4\"\n",
    "frame_rate = 15\n",
    "num_frames = 500\n",
    "\n",
    "def create_video():\n",
    "    with open(annotations_path_v2, 'r') as f:\n",
    "        annotations = json.load(f)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, frame_rate, (512, 256))\n",
    "\n",
    "    start_idx = random.randint(0, max(0, len(annotations) - num_frames))\n",
    "\n",
    "    kalman_filters_gt = [initialize_kalman() for _ in range(34)]\n",
    "    kalman_filters_pred = [initialize_kalman() for _ in range(34)]\n",
    "\n",
    "    for annotation in annotations[start_idx:start_idx + num_frames]:\n",
    "        image_name = annotation['Image'] + '.jpg'\n",
    "        image_path_v2 = os.path.join(image_dir_v2, image_name)\n",
    "        image_path_v3 = os.path.join(image_dir_v3, image_name)\n",
    "\n",
    "        if not os.path.exists(image_path_v2) or not os.path.exists(image_path_v3):\n",
    "            print(f\"Imagen no encontrada: {image_name}\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(image_path_v2)\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "\n",
    "        pose_gt = sum(annotation['Pose1'], []) + sum(annotation['Pose2'], [])  # Aplanar lista\n",
    "        image_gt = draw_pose(image.copy(), pose_gt, kalman_filters_gt, fighter_colors)\n",
    "\n",
    "        image_pil = remove_background(Image.open(image_path_v3).resize((256, 256)))\n",
    "        image_tensor = data_transforms(image_pil).unsqueeze(0).to(device)\n",
    "        pose_pred = model(image_tensor).cpu().detach().numpy().flatten()\n",
    "\n",
    "        image_pred = draw_pose(image.copy(), pose_pred, kalman_filters_pred, fighter_colors)\n",
    "\n",
    "        video_writer.write(np.hstack((image_gt, image_pred)))\n",
    "\n",
    "    video_writer.release()\n",
    "    print(f'Video guardado en {output_video_path}')\n",
    "\n",
    "# Ejecutar\n",
    "create_video()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
